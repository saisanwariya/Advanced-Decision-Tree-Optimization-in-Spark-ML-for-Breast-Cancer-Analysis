{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">Advanced Decision Tree Optimization in Spark ML for <font color=\"red\">Breast Cancer Analysis</font></h1>  \n",
    "<h2 style=\"text-align:center\">Sai Sanwariya Narayan</h2>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and set up the Python files for this Lab\n",
    "1. The decision_tree_plot directory\n",
    "- decision_tree_parser.py\n",
    "- decision_tree_plot.py\n",
    "- tree_template.jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following two lines import relevant functions from the two python files you uploaded into the decision_tree_plot subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_tree_plot.decision_tree_parser import decision_tree_parse\n",
    "from decision_tree_plot.decision_tree_plot import plot_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first part of this project runs Spark in the local mode (with a small hyperparameter tuning)\n",
    "## The second part of this project will require a large hyperparameter tuning in cluster mode\n",
    "## Notice we are creating a SparkSession, not a SparkContext, when we use ML pipeline.\n",
    "## The \"getOrCreate()\" method means we can re-evaluate this without a need to \"stop the current SparkSession\" first (unlike SparkContext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=SparkSession.builder.master(\"local\").appName(\"lab 8 DT\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completing the following path with the path for your home directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_schema = StructType([ StructField(\"id\", IntegerType(), False ), \\\n",
    "                        StructField(\"clump_thickness\", IntegerType(), False), \\\n",
    "                        StructField(\"unif_cell_size\", IntegerType(), False ), \\\n",
    "                        StructField(\"unif_cell_shape\", IntegerType(), False ), \\\n",
    "                        StructField(\"marg_adhesion\", IntegerType(), False), \\\n",
    "                        StructField(\"single_epith_cell_size\", IntegerType(), False), \\\n",
    "                        StructField(\"bare_nuclei\", IntegerType(), False),\\\n",
    "                        StructField(\"bland_chrom\", IntegerType(), False), \\\n",
    "                        StructField(\"norm_nucleoli\", IntegerType(), False), \\\n",
    "                        StructField(\"mitoses\", IntegerType(), False), \\\n",
    "                        StructField(\"class\", StringType(), False) \\\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ss.read.csv(\"/storage/home/breast-cancer-wisconsin.data.txt\", schema=bc_schema, header=True, inferSchema=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation Using DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- clump_thickness: integer (nullable = true)\n",
      " |-- unif_cell_size: integer (nullable = true)\n",
      " |-- unif_cell_shape: integer (nullable = true)\n",
      " |-- marg_adhesion: integer (nullable = true)\n",
      " |-- single_epith_cell_size: integer (nullable = true)\n",
      " |-- bare_nuclei: integer (nullable = true)\n",
      " |-- bland_chrom: integer (nullable = true)\n",
      " |-- norm_nucleoli: integer (nullable = true)\n",
      " |-- mitoses: integer (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "|     id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "|1000025|              5|             1|              1|            1|                     2|          1|          3|            1|      1|    2|\n",
      "|1002945|              5|             4|              4|            5|                     7|         10|          3|            2|      1|    2|\n",
      "|1015425|              3|             1|              1|            1|                     2|          2|          3|            1|      1|    2|\n",
      "|1016277|              6|             8|              8|            1|                     3|          4|          3|            7|      1|    2|\n",
      "|1017023|              4|             1|              1|            3|                     2|          1|          3|            1|      1|    2|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|    4|  241|\n",
      "|    2|  458|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "class_count = data.groupBy(col(\"class\")).count()\n",
    "class_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting and Filtering Rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "|     id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "|1057013|              8|             4|              5|            1|                     2|       null|          7|            3|      1|    4|\n",
      "|1096800|              6|             6|              6|            9|                     6|       null|          7|            8|      1|    2|\n",
      "|1183246|              1|             1|              1|            1|                     1|       null|          2|            1|      1|    2|\n",
      "|1184840|              1|             1|              3|            1|                     2|       null|          2|            1|      1|    2|\n",
      "|1193683|              1|             1|              2|            1|                     3|       null|          1|            1|      1|    2|\n",
      "|1197510|              5|             1|              1|            1|                     2|       null|          3|            1|      1|    2|\n",
      "|1241232|              3|             1|              4|            1|                     2|       null|          3|            1|      1|    2|\n",
      "| 169356|              3|             1|              1|            1|                     2|       null|          3|            1|      1|    2|\n",
      "| 432809|              3|             1|              3|            1|                     2|       null|          2|            1|      1|    2|\n",
      "| 563649|              8|             8|              8|            1|                     2|       null|          6|           10|      1|    4|\n",
      "| 606140|              1|             1|              1|            1|                     2|       null|          2|            1|      1|    2|\n",
      "|  61634|              5|             4|              3|            1|                     2|       null|          2|            3|      1|    2|\n",
      "| 704168|              4|             6|              5|            6|                     7|       null|          4|            9|      1|    2|\n",
      "| 733639|              3|             1|              1|            1|                     2|       null|          3|            1|      1|    2|\n",
      "|1238464|              1|             1|              1|            1|                     1|       null|          2|            1|      1|    2|\n",
      "|1057067|              1|             1|              1|            1|                     1|       null|          1|            1|      1|    2|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.filter(col(\"bare_nuclei\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data.filter(col(\"bare_nuclei\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "683"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "699"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|    4|  239|\n",
      "|    2|  444|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "class_count2 = data2.groupBy(col(\"class\")).count()\n",
    "class_count2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StringIndex\n",
    "- Transforms a column of string to a new column of index (type double).\n",
    "- The feature transformation involves three steps:\n",
    "-- Step 1: Create a \"transformer\" \n",
    "-- Step 2: Use the data (which contains all possible values of the string column) to create a mapping (of these strings into an integer/index)\n",
    "-- Step 3: Use the mapping to generate the new column's value (i.e., trasformed index) for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer= StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexerModel: uid=StringIndexer_4a9b224e0113, handleInvalid=error"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = labelIndexer.transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+\n",
      "|     id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|indexedLabel|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+\n",
      "|1000025|              5|             1|              1|            1|                     2|          1|          3|            1|      1|    2|         0.0|\n",
      "|1002945|              5|             4|              4|            5|                     7|         10|          3|            2|      1|    2|         0.0|\n",
      "|1015425|              3|             1|              1|            1|                     2|          2|          3|            1|      1|    2|         0.0|\n",
      "|1016277|              6|             8|              8|            1|                     3|          4|          3|            7|      1|    2|         0.0|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion', \\\n",
    "                  'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_e858ceeeb680"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = assembler.transform(transformed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will use `vectorized_data` for splitting labelled data into training data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|indexedLabel|\n",
      "+--------------------+------------+\n",
      "|[5.0,1.0,1.0,1.0,...|         0.0|\n",
      "|[5.0,4.0,4.0,5.0,...|         0.0|\n",
      "|[3.0,1.0,1.0,1.0,...|         0.0|\n",
      "|[6.0,8.0,8.0,1.0,...|         0.0|\n",
      "|[4.0,1.0,1.0,3.0,...|         0.0|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorized2_data = vectorized_data.select(\"features\",'indexedLabel')\n",
    "vectorized2_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Learning and Evaluation (1 hyperparameter setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomSplit is a method for DataFrames that splits data in the DataFrame into two subsets, one for training, the other for testing, using a number as the seed for random number generator.\n",
    "## If you want to generate a different split, you can use a different seed (preferably a prime number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData= vectorized_data.randomSplit([0.75, 0.25], seed=1237)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"indexedLabel\", maxDepth=6, minInstancesPerNode=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier_b959414ba3e0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = dt.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_b959414ba3e0, depth=6, numNodes=31, numClasses=2, numFeatures=9"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = dt_model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+--------------------+-------------+--------------------+----------+\n",
      "|    id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|indexedLabel|            features|rawPrediction|         probability|prediction|\n",
      "+------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+--------------------+-------------+--------------------+----------+\n",
      "|142932|              7|             6|             10|            5|                     3|         10|          9|           10|      2|    4|         1.0|[7.0,6.0,10.0,5.0...|  [1.0,118.0]|[0.00840336134453...|       1.0|\n",
      "|144888|              8|            10|             10|            8|                     5|         10|          7|            8|      1|    4|         1.0|[8.0,10.0,10.0,8....|  [1.0,118.0]|[0.00840336134453...|       1.0|\n",
      "|183913|              1|             2|              2|            1|                     2|          1|          1|            1|      1|    2|         0.0|[1.0,2.0,2.0,1.0,...|  [299.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "+------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+------------+--------------------+-------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prediction.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the actual labels and predicted labels more easily, we can select the following columns.\n",
    "## The `probability` column records the probability for the row to be in the \"zero/benign class\" and the probability to be in the \"one/malignant class\".\n",
    "## The `prediction` column records the predicted label for each row, which is the class with the higher probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+--------------------+----------+\n",
      "|            features|class|indexedLabel|         probability|prediction|\n",
      "+--------------------+-----+------------+--------------------+----------+\n",
      "|[7.0,6.0,10.0,5.0...|    4|         1.0|[0.00840336134453...|       1.0|\n",
      "|[8.0,10.0,10.0,8....|    4|         1.0|[0.00840336134453...|       1.0|\n",
      "|[1.0,2.0,2.0,1.0,...|    2|         0.0|           [1.0,0.0]|       0.0|\n",
      "|[5.0,3.0,2.0,8.0,...|    4|         1.0|           [1.0,0.0]|       0.0|\n",
      "|[10.0,4.0,4.0,10....|    4|         1.0|[0.04166666666666...|       1.0|\n",
      "+--------------------+-----+------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prediction.select(\"features\",\"class\",\"indexedLabel\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '4']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelIndexer.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter=IndexToString(inputCol=\"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_prediction = labelConverter.transform(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+----------+--------------+\n",
      "|            features|class|indexedLabel|prediction|predictedClass|\n",
      "+--------------------+-----+------------+----------+--------------+\n",
      "|[7.0,6.0,10.0,5.0...|    4|         1.0|       1.0|             4|\n",
      "|[8.0,10.0,10.0,8....|    4|         1.0|       1.0|             4|\n",
      "|[1.0,2.0,2.0,1.0,...|    2|         0.0|       0.0|             2|\n",
      "|[5.0,3.0,2.0,8.0,...|    4|         1.0|       0.0|             2|\n",
      "|[10.0,4.0,4.0,10....|    4|         1.0|       1.0|             4|\n",
      "+--------------------+-----+------------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2_prediction.select(\"features\",\"class\",\"indexedLabel\",\"prediction\",\"predictedClass\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.9338310711681739\n"
     ]
    }
   ],
   "source": [
    "f1 = evaluator.evaluate(test_prediction)\n",
    "print(\"f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DT Learning Using ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a decision tree (using pipeline) and compute f1 measure of the testing data.\n",
    "- Record the f1 measure for the max_depth below  \n",
    "- Recommended value for maxDepth: 2 to 10\n",
    "- Recommended value for minInstancesPerNode: 1 to 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData= data2.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer= StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)\n",
    "assembler = VectorAssembler( inputCols=input_features, outputCol=\"features\")\n",
    "dt=DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth= 6, minInstancesPerNode= 3)\n",
    "predictionConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "pipeline = Pipeline(stages=[labelIndexer, assembler, dt, predictionConverter])\n",
    "model = pipeline.fit(trainingData)\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_20574e130de4"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_2d850a5fcbe7"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+--------------+\n",
      "|class|indexedLabel|prediction|predictedClass|\n",
      "+-----+------------+----------+--------------+\n",
      "|    4|         1.0|       1.0|             4|\n",
      "|    4|         1.0|       1.0|             4|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "|    4|         1.0|       1.0|             4|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "|    4|         1.0|       0.0|             2|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "|    4|         1.0|       1.0|             4|\n",
      "+-----+------------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"class\",\"indexedLabel\",\"prediction\",\"predictedClass\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.9439967439967439\n"
     ]
    }
   ],
   "source": [
    "f1 = evaluator.evaluate(predictions)\n",
    "print(\"f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stages[2] of the pipeline is \"dt\" (our model - a DecisionTreeClassifier). \n",
    "## model is a DataFrame representing a trained pipeline.\n",
    "## model.stages[2] gives us the Decision Tree model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_9ba308cc3902, depth=6, numNodes=29, numClasses=2, numFeatures=9\n"
     ]
    }
   ],
   "source": [
    "DTmodel = model.stages[2]\n",
    "print(DTmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"/storage/home/DTmodel_vis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree=decision_tree_parse(DTmodel, ss, model_path)\n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])\n",
    "plot_trees(tree, column = column, output_path = '/storage/home/DTtree_local.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Hyperparameter Tuning for Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion', \\\n",
    "                  'single_epith_cell_size', 'bare_nuclei', 'bland_chrom', 'norm_nucleoli', 'mitoses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testingData= data2.randomSplit([0.8, 0.2], seed=1237)\n",
    "model_path=\"/storage/home/DT_HPT_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is  4 , best minInstancesPerNode =  3 , testing f1 =  0.9699248120300752\n"
     ]
    }
   ],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['max_depth', 'minInstancesPerNode', 'training f1', 'testing f1', 'Best Model'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "highest_testing_f1 = 0\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "max_depth_list = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "minInstancesPerNode_list = [2, 3, 4, 5, 6]\n",
    "labelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)\n",
    "assembler = VectorAssembler( inputCols=input_features, outputCol=\"features\")\n",
    "labelConverter = IndexToString(inputCol = \"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "trainingData.persist()\n",
    "testingData.persist()\n",
    "for max_depth in max_depth_list:\n",
    "    for minInsPN in minInstancesPerNode_list:\n",
    "        seed = 37\n",
    "        # Construct a DT model using a set of hyper-parameter values and training data\n",
    "        dt= DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth= max_depth , minInstancesPerNode= minInsPN)\n",
    "        pipeline = Pipeline(stages=[labelIndexer, assembler, dt, predictionConverter])\n",
    "        model = pipeline.fit(trainingData)\n",
    "        training_predictions = model.transform(trainingData)\n",
    "        testing_predictions = model.transform(testingData)\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        training_f1 = evaluator.evaluate(training_predictions)\n",
    "        testing_f1 = evaluator.evaluate(testing_predictions)\n",
    "        # We use 0 as default value of the 'Best Model' column in the Pandas DataFrame.\n",
    "        # The best model will have a value 1000\n",
    "        hyperparams_eval_df.loc[index] = [ max_depth, minInsPN, training_f1, testing_f1, 0]  \n",
    "        index = index +1\n",
    "        if testing_f1 > highest_testing_f1 :\n",
    "            best_max_depth = max_depth\n",
    "            best_minInsPN = minInsPN\n",
    "            best_index = index -1\n",
    "            best_parameters_training_f1 = training_f1\n",
    "            best_DTmodel= model.stages[2]\n",
    "            best_tree = decision_tree_parse(best_DTmodel, ss, model_path)\n",
    "            column = dict( [ (str(idx), i) for idx, i in enumerate(input_features) ])           \n",
    "            highest_testing_f1 = testing_f1\n",
    "print('The best max_depth is ', best_max_depth, ', best minInstancesPerNode = ', \\\n",
    "      best_minInsPN, ', testing f1 = ', highest_testing_f1) \n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path=\"/storage/home/DT_HPT_local_best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_tree=decision_tree_parse(best_DTmodel, ss, best_model_path)\n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])\n",
    "plot_trees(best_tree, column = column, output_path = '/storage/home/DT_HPT_local_best.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_max_depth, best_minInsPN, best_parameters_training_f1, highest_testing_f1, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/storage/home/HPT_Local.csv\"\n",
    "hyperparams_eval_df.to_csv(output_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Revised Hyper-parameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The range of hyper-parameter tuning:\n",
    "- max_depth: 2 to 11\n",
    "- minInstancesPerNode: 2 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best max_depth is  4 , best minInstancesPerNode =  3 , testing f1 =  0.9699248120300752\n"
     ]
    }
   ],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['max_depth', 'minInstancesPerNode', 'training f1', 'testing f1', 'Best Model'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "highest_testing_f1 = 0\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "max_depth_list = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "minInstancesPerNode_list = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "labelIndexer = StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data2)\n",
    "assembler = VectorAssembler( inputCols=input_features, outputCol=\"features\")\n",
    "labelConverter = IndexToString(inputCol = \"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "trainingData.persist()\n",
    "testingData.persist()\n",
    "for max_depth in max_depth_list:\n",
    "    for minInsPN in minInstancesPerNode_list:\n",
    "        seed = 37\n",
    "        # Construct a DT model using a set of hyper-parameter values and training data\n",
    "        dt= DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth= max_depth , minInstancesPerNode= minInsPN)\n",
    "        pipeline = Pipeline(stages=[labelIndexer, assembler, dt, predictionConverter])\n",
    "        model = pipeline.fit(trainingData)\n",
    "        training_predictions = model.transform(trainingData)\n",
    "        testing_predictions = model.transform(testingData)\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        training_f1 = evaluator.evaluate(training_predictions)\n",
    "        testing_f1 = evaluator.evaluate(testing_predictions)\n",
    "        # We use 0 as default value of the 'Best Model' column in the Pandas DataFrame.\n",
    "        # The best model will have a value 1000\n",
    "        hyperparams_eval_df.loc[index] = [ max_depth, minInsPN, training_f1, testing_f1, 0]  \n",
    "        index = index +1\n",
    "        if testing_f1 > highest_testing_f1 :\n",
    "            best_max_depth = max_depth\n",
    "            best_minInsPN = minInsPN\n",
    "            best_index = index -1\n",
    "            best_parameters_training_f1 = training_f1\n",
    "            best_DTmodel= model.stages[2]\n",
    "            best_tree = decision_tree_parse(best_DTmodel, ss, model_path)\n",
    "            column = dict( [ (str(idx), i) for idx, i in enumerate(input_features) ])           \n",
    "            highest_testing_f1 = testing_f1\n",
    "print('The best max_depth is ', best_max_depth, ', best minInstancesPerNode = ', \\\n",
    "      best_minInsPN, ', testing f1 = ', highest_testing_f1) \n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path=\"/storage/home/DT_HPT_local_best\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_tree=decision_tree_parse(best_DTmodel, ss, best_model_path)\n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])\n",
    "plot_trees(best_tree, column = column, output_path = '/storage/home/DT_HPT_local_best.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_max_depth, best_minInsPN, best_parameters_training_f1, highest_testing_f1, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/storage/home/Lab8_HPT_Local.csv\"\n",
    "hyperparams_eval_df.to_csv(output_path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
